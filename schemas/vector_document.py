from utils.flow_logger import function_logger
"""
PHASE 3 â€” Vector Document Schema

Contract for all knowledge chunks stored in the vector database.
Every chunk MUST conform to this schema to ensure:
- Searchability
- Filterability  
- Explainability
- Consistency
"""

from dataclasses import dataclass, field
from datetime import datetime
from typing import Optional
from enum import Enum


class SourceType(str, Enum):
    """Type of knowledge source."""
    SYLLABUS = "syllabus"
    OUTLINE = "outline"
    UPLOADED_PDF = "uploaded_pdf"
    INSTITUTIONAL = "institutional"
    EXAMPLE = "example"


class UploadedBy(str, Enum):
    """Origin of the document."""
    SYSTEM = "system"
    ADMIN = "admin"
    USER = "user"


@dataclass
class VectorDocumentMetadata:
    """
    Mandatory metadata for every chunk.
    Enables filtering, tracing, and explainability.
    """
    
    institution_name: str
    """Institution or organization name (e.g., 'MIT', 'Example University')"""
    
    degree_level: str
    """Academic level (e.g., 'undergraduate', 'graduate', 'professional')"""
    
    subject_domain: str
    """Primary subject area (e.g., 'computer_science', 'engineering')"""
    
    audience_level: str
    """Target learner level (e.g., 'beginner', 'intermediate', 'advanced')"""
    
    depth_level: str
    """Depth requirement (e.g., 'foundational', 'intermediate', 'advanced')"""
    
    source_type: SourceType
    """Type of source: syllabus, outline, uploaded_pdf, institutional, example"""
    
    uploaded_by: UploadedBy
    """Who uploaded: system, admin, user"""
    
    timestamp: datetime = field(default_factory=datetime.utcnow)
    """When this chunk was stored"""
    
    source_name: Optional[str] = None
    """Original filename or document name"""
    
    original_url: Optional[str] = None
    """Original source URL if applicable"""
    
    session_id: Optional[str] = None
    """Session ID if from user upload"""


@dataclass
class VectorDocument:
    """
    A single chunk of knowledge to be stored in ChromaDB.
    
    Rules:
    - content must be 500-800 tokens (roughly)
    - metadata is ALWAYS required
    - embedding computed separately by embedding service
    - id is auto-generated by ChromaDB
    """
    
    content: str
    """The actual text chunk (plain text, not markdown)"""
    
    metadata: VectorDocumentMetadata
    """Mandatory metadata for filtering and tracing"""
    
    document_id: Optional[str] = None
    """Unique identifier (auto-generated by ChromaDB if None)"""
    
    embedding: Optional[list] = None
    """Embedding vector (computed by embedding service, not stored)"""
    
    chunk_index: int = 0
    """Sequential chunk number from original document"""
    
    @function_logger("Execute validate")
    def validate(self):
        """Ensure document meets all requirements."""
        if not self.content or len(self.content.strip()) == 0:
            raise ValueError("Content cannot be empty")
        
        if len(self.content.split()) < 50:
            raise ValueError("Content too short (minimum 50 words)")
        
        if len(self.content.split()) > 2000:
            raise ValueError("Content too long (maximum 2000 words)")
        
        if not self.metadata:
            raise ValueError("Metadata is mandatory")
        
        return True
    
    @function_logger("Execute to chroma format")
    def to_chroma_format(self) -> dict:
        """
        Convert to ChromaDB storage format.
        
        Returns:
            dict with keys: id, document, metadatas
        """
        metadata_dict = {
            "institution_name": self.metadata.institution_name,
            "degree_level": self.metadata.degree_level,
            "subject_domain": self.metadata.subject_domain,
            "audience_level": self.metadata.audience_level,
            "depth_level": self.metadata.depth_level,
            "source_type": self.metadata.source_type.value,
            "uploaded_by": self.metadata.uploaded_by.value,
            "timestamp": self.metadata.timestamp.isoformat(),
            "chunk_index": str(self.chunk_index),
        }
        
        if self.metadata.source_name:
            metadata_dict["source_name"] = self.metadata.source_name
        
        if self.metadata.original_url:
            metadata_dict["original_url"] = self.metadata.original_url
        
        if self.metadata.session_id:
            metadata_dict["session_id"] = self.metadata.session_id
        
        return {
            "id": self.document_id or f"doc_{hash(self.content) % 10**8}",
            "document": self.content,
            "metadatas": metadata_dict,
        }
    
    @staticmethod
    @function_logger("Execute from chroma format")
    def from_chroma_format(chroma_doc: dict) -> "VectorDocument":
        """
        Reconstruct from ChromaDB storage format.
        
        Args:
            chroma_doc: dict from ChromaDB query result
            
        Returns:
            VectorDocument instance
        """
        meta = chroma_doc.get("metadatas", {})
        
        metadata = VectorDocumentMetadata(
            institution_name=meta.get("institution_name", "unknown"),
            degree_level=meta.get("degree_level", "unknown"),
            subject_domain=meta.get("subject_domain", "unknown"),
            audience_level=meta.get("audience_level", "unknown"),
            depth_level=meta.get("depth_level", "unknown"),
            source_type=SourceType(meta.get("source_type", "example")),
            uploaded_by=UploadedBy(meta.get("uploaded_by", "system")),
            timestamp=datetime.fromisoformat(meta.get("timestamp", datetime.utcnow().isoformat())),
            source_name=meta.get("source_name"),
            original_url=meta.get("original_url"),
            session_id=meta.get("session_id"),
        )
        
        return VectorDocument(
            content=chroma_doc.get("document", ""),
            metadata=metadata,
            document_id=chroma_doc.get("id"),
            chunk_index=int(meta.get("chunk_index", 0)),
        )
