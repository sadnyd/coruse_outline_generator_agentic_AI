"""
ðŸ“‹ COMPREHENSIVE PHASED IMPLEMENTATION PLAN

Course AI Agent: From Bare Bones to Full Agentic System

==============================================================================
ðŸŸ¢ PHASE 0 â€” Project Skeleton & Contracts (Foundation)
==============================================================================

ðŸŽ¯ GOAL
Create nothing intelligent yet â€” only structure, interfaces, and contracts.
Ensure future agents plug in cleanly without architectural rot.

DELIVERABLES
âœ“ Structured directory layout
âœ“ Pydantic schema contracts (input, output, intermediate)
âœ“ Agent base classes with run() signatures
âœ“ Test scaffolding (no tests pass yet)
âœ“ Stub agent implementations

RESPONSIBILITIES (Who Owns What)
- Project Lead: Directory structure, schema ownership
- Backend Lead: Agent base classes, interface design
- QA Lead: Test scaffolding design

KEY FILES
- schemas/user_input.py - UserInputSchema contract
- schemas/course_outline.py - CourseOutlineSchema + LearningObjective contract
- schemas/agent_outputs.py - Per-agent output contracts
- agents/base.py - BaseAgent + agent responsibilities
- tests/ - All test skeletons with docstrings

EXIT CONDITION
âœ“ All imports work
âœ“ Agents can be instantiated
âœ“ Schemas validate correctly
âœ“ No runtime errors

TESTING
- test_schemas.py: Schema validation (Pydantic, required fields)
- test_project_boot.py: Import checks, directory existence

DURATION ESTIMATE: 1-2 days (one engineer)

---

ðŸŸ¢ PHASE 1 â€” Streamlit UI + Session Management
==============================================================================

ðŸŽ¯ GOAL
Build the full frontend input experience.
Establish session-aware request context (no AI yet).

DELIVERABLES
âœ“ Streamlit UI with all input fields
âœ“ Session management (in-memory store)
âœ“ PDF upload â†’ temp storage lifecycle
âœ“ Form validation on submit
âœ“ Input capture in structured schema

RESPONSIBILITIES
- Frontend Lead: Streamlit UI, form layout, UX
- Backend Lead: Session manager, temp file handling
- DevOps: Temp directory configuration (cleanup)

KEY FILES
- app.py - Streamlit entry point
- utils/session.py - SessionManager class
- tests/test_phase_1_ui.py - UI + session tests

AGENT STUBS NEEDED
- Session storage only (no agent invocation yet)

SESSION CONTEXT (Established Now)
```
{
  "session_id": "uuid",
  "user_input": UserInputSchema,
  "pdf_path": "/tmp/session_123.pdf",
  "intermediate_results": {},
  "conversation_history": []
}
```

EXIT CONDITION
âœ“ Form submits with valid input
âœ“ Form rejects incomplete input
âœ“ PDF uploaded to temp directory
âœ“ Session persists across requests
âœ“ Reset button clears all data
âœ“ temp files cleaned up after session

TESTING
- test_phase_1_ui.py: Form capture, validation, session lifecycle

DURATION ESTIMATE: 3-4 days

---

ðŸŸ¢ PHASE 2 â€” Orchestrator (Single-Pass, Non-Agentic)
==============================================================================

ðŸŽ¯ GOAL
Create end-to-end vetical slice: input â†’ orchestrator â†’ module creation â†’ output.
Still non-agentic (no loops, retries, or validation yet).

DELIVERABLES
âœ“ Orchestrator accepts UserInputSchema
âœ“ Orchestrator calls Module Creation Agent (stubbed, returns mock outline)
âœ“ Module Creation Agent respects constraints (duration, depth, audience)
âœ“ Orchestrator returns valid CourseOutlineSchema
âœ“ UI displays generated outline

RESPONSIBILITIES
- Backend Lead: Orchestrator logic, agent dispatch
- Module Creation Owner: Initialization of core agent
- DevOps: LLM config (which model, API keys)

KEY FILES
- agents/orchestrator.py - CourseOrchestratorAgent implementation
- agents/module_creation_agent.py - Initial version (stateless generator)
- app.py - UI + orchestrator integration

AGENT IMPLEMENTATIONS
- ModuleCreationAgent: Mock/template-based outline generator
- All others: Still stubs

ORCHESTRATOR LOGIC (Simplified)
```
1. Receive UserInputSchema from UI
2. Validate constraints
3. Call ModuleCreationAgent(user_input + empty retrieval + empty web results)
4. Return CourseOutlineSchema to UI
```

EXIT CONDITION
âœ“ Full pipeline works: UI â†’ Orchestrator â†’ Module Agent â†’ UI
âœ“ Outline respects duration constraint
âœ“ Outline respects depth and audience
âœ“ Output conforms to CourseOutlineSchema
âœ“ No external API calls yet (LLM mocked or simple template)

TESTING
- test_phase_2_orchestrator.py: Constraint checks, schema compliance

DURATION ESTIMATE: 4-5 days

---

ðŸŸ¢ PHASE 3 â€” Retrieval Agent & ChromaDB (Private Knowledge)
==============================================================================

ðŸŽ¯ GOAL
Add institutional knowledge retrieval (RAG).
Independent from other agents â€” can be tested in isolation.

DELIVERABLES
âœ“ ChromaDB initialized with sample curricula
âœ“ Document chunking & embedding pipeline
âœ“ Similarity search with metadata filtering
âœ“ Retrieval Agent calls vector DB autonomously
âœ“ Orchestrator calls Retrieval Agent in parallel

RESPONSIBILITIES
- RAG/ML Lead: ChromaDB setup, chunking strategy, QA of retrieval
- Backend Lead: Retrieval Agent implementation
- Data Lead: Sample curriculum ingestion

KEY FILES
- vectorstore/chroma_client.py - ChromaDB connector
- vectorstore/embeddings.py - Embedding provider (LangChain wrapper)
- agents/retrieval_agent.py - Autonomous retrieval logic
- data/sample_curricula/ - Sample docs for testing

DATA INGESTION
- Load sample curriculum PDFs (2-5 public syllabi)
- Chunk into ~500-token segments
- Embed using LangChain embeddings (OpenAI / Anthropic / local)
- Store in ChromaDB with metadata (institution, degree, year, tags)

RETRIEVAL AGENT AUTONOMY
Receives user_input â†’ formulates query (topic + audience + depth) â†’ searches DB

EXIT CONDITION
âœ“ ChromaDB has 5+ sample curricula (10K+ chunks)
âœ“ Similarity search returns relevant chunks
âœ“ Metadata filters work (institution, degree, year)
âœ“ Retrieval Agent is autonomous (no orchestrator instruction on what to search)
âœ“ Output conforms to RetrievalAgentOutput schema
âœ“ Orchestrator calls Retrieval Agent in parallel with Module Creation

TESTING
- test_phase_3_retrieval.py: Search accuracy, metadata filters, autonomy

DURATION ESTIMATE: 5-6 days

---

ðŸŸ¢ PHASE 4 â€” Web Search Agent (Public Knowledge)
==============================================================================

ðŸŽ¯ GOAL
Add external knowledge via web search (isolated).
Fallback strategy ensures resilience.

DELIVERABLES
âœ“ LangChain tool wrappers for Tavily, DuckDuckGo, SerpAPI
âœ“ Web Search Agent formulates queries autonomously
âœ“ Fallback logic when primary tool underperforms
âœ“ Results ranked by confidence
âœ“ Orchestrator calls Web Search Agent in parallel
âœ“ No hallucinated URLs

RESPONSIBILITIES
- Tool Integration Lead: LangChain tool setup
- Backend Lead: Web Search Agent, fallback logic
- QA Lead: URL validation, hallucination checks

KEY FILES
- tools/web_tools.py - Web search tool wrappers
- agents/web_search_agent.py - Autonomous query construction + fallback

TOOL STRATEGY
1. Try Tavily (most reliable for academia-oriented results)
2. Fallback to DuckDuckGo (privacy, no API key)
3. Fallback to SerpAPI (if configured)
4. Threshold: accept if top-3 results have relevance score > 0.6

EXIT CONDITION
âœ“ All three tools can be invoked
âœ“ Fallback triggers when primary results poor
âœ“ URLs in output are not hallucinated (spot-check)
âœ“ Confidence scores > 0.5
âœ“ Web Search Agent autonomous (formulates query from user_input)
âœ“ Orchestrator calls in parallel with Retrieval Agent

TESTING
- test_phase_4_web_search.py: Tool invocation, fallback, hallucination checks

DURATION ESTIMATE: 4-5 days

---

ðŸŸ¢ PHASE 5 â€” Module Creation Agent (Core Synthesis)
==============================================================================

ðŸŽ¯ GOAL
Implement the brain of the system.
Consumes all inputs (user choice + retrieved docs + web results + PDF).
Generates structured, constraint-respecting course outline.

DELIVERABLES
âœ“ Full Module Creation Agent implementation
âœ“ Prompt templates (Bloom-aware, backward design)
âœ“ Constraint enforcement (duration, depth, audience, learning_mode)
âœ“ Objective generation (measurable Bloom verbs)
âœ“ Assessment alignment
âœ“ Lesson breakdown with activities & resources
âœ“ Provenance tracking (which input influenced which module)
âœ“ Multiple candidate generation (optional: Theory-Oriented vs Project-Based)

RESPONSIBILITIES
- Curriculum/Instructional Designer Lead: Prompt templates, heuristics
- Backend Lead: Agent orchestration, constraint logic
- Content Lead: Validation of pedagogical soundness

KEY FILES
- agents/module_creation_agent.py - Full implementation
- prompts/module_creator.txt - Prompt template
- utils/scoring.py - Constraint validation helpers

INPUTS TO AGENT
```
{
  "user_input": UserInputSchema,
  "retrieved_docs": [RetrievedChunk],
  "web_results": [WebSearchResult],
  "pdf_content": Optional[str]
}
```

AGENT RESPONSIBILITIES
1. Parse inputs and extract key themes
2. Generate module structure
   - Count: scales with duration (e.g., 40 hrs â†’ 5-6 modules)
   - Flow: follows pedagogical progression (simple â†’ complex)
3. For each module, generate:
   - Title & synopsis
   - 3-7 learning objectives (Bloom-mapped, measurable)
   - Lesson breakdown (lessons, activities, timings)
   - Assessment (aligned to objectives)
   - Resources (with provenance)
4. Respect constraints:
   - Total hours = duration_hours
   - Learning_mode affects structure (async = more self-study, sync = more discussion)
   - Depth requirement affects content level (Conceptual = overview; Implementation = hands-on)
   - Audience level affects language/prerequisites
5. Track provenance (web result â†’ module mapping, retrieved doc â†’ lesson mapping)

OPTIONAL: GENERATE CANDIDATES
If time permits, generate 2-3 variants:
- Theory-Oriented (more lectures, fewer labs)
- Project-Based (fewer lectures, capstone project)
- Industry-Focused (real-world case studies)
Let validator rank them (PHASE 6).

EXIT CONDITION
âœ“ Objectives use proper Bloom verbs (Explain, Implement, Analyze, etc.)
âœ“ All objectives marked with Bloom level
âœ“ Module count scales with duration
âœ“ Learning mode affects structure (verifiable)
âœ“ Depth requirement affects content
âœ“ Audience level affects language
âœ“ Total hours sum matches requested duration
âœ“ Each module has 3-7 objectives
âœ“ Each objective has aligned assessment
âœ“ Provenance tracked for all resources
âœ“ Output is valid CourseOutlineSchema

TESTING
- test_phase_5_module_creation.py: Constraint checks, Bloom mapping, alignment

DURATION ESTIMATE: 8-10 days

---

ðŸŸ¢ PHASE 6 â€” Validator Agent (Quality Gate + Loop)
==============================================================================

ðŸŽ¯ GOAL
Introduce self-correction & agentic behavior (CRITICAL PHASE).
This is what turns a "prompt-based app" into an "agent system."

DELIVERABLES
âœ“ Rubric-based scoring (0-100)
âœ“ Scoring breakdown by category
âœ“ Targeted feedback generation
âœ“ Accept/Reject decision (threshold 75)
âœ“ Regeneration signal + Max retries
âœ“ Orchestrator retry loop

RESPONSIBILITIES
- Rubric Design Lead: Scoring criteria, calibration
- Backend Lead: Validator Agent, retry loop logic
- QA Lead: Score calibration on golden set of outlines

KEY FILES
- agents/validator_agent.py - Validator implementation
- utils/scoring.py - Rubric scoring logic
- agents/orchestrator.py - Retry loop integration

VALIDATOR RUBRIC (Total 100 points, Threshold 75)
```
Coverage & Coherence (0-25):
  - Are all expected topics represented?
  - Is the flow logical (prerequisites â†’ building blocks â†’ synthesis)?
  - Check: no orphan modules, no circular prerequisites

Audience Alignment (0-20):
  - Does depth match audience level?
  - Is language appropriate?
  - Check: undergrad outline != postgrad outline for same topic

Depth & Technical Accuracy (0-20):
  - Are concepts technically correct?
  - Is depth appropriate?
  - Check: Implementation-level has code examples; Conceptual doesn't

Assessability (0-15):
  - Are learning objectives measurable?
  - Are assessments aligned to objectives (1:1 mapping)?
  - Check: each objective has â‰¥1 assessment

Practicality / Feasibility (0-10):
  - Is pacing realistic?
  - Total hours vs content depth compatible?
  - Check: 40-hr course doesn't have 200 hrs of required reading

Originality & Duplication (0-10):
  - Is the structure unique (adds value vs copy)?
  - No plagiarism / hallucinated sources?
  - Check: web URLs are real (spot check)
```

VALIDATOR FEEDBACK FORMAT
```
{
  "score": 78,
  "rubric_breakdown": {
    "coverage": 22,
    "audience_alignment": 18,
    "depth_accuracy": 19,
    "assessability": 12,
    "practicality": 8,
    "originality": -1 // failed plagiarism check
  },
  "accept": true,  // score >= 75
  "feedback": [
    "Module 2 needs implementation examples (score: 12/15 on Technical Accuracy)",
    "Add assessment for LO 2.3 (Implement X) â€” currently unmeasured"
  ],
  "targeted_edits": {
    "module_2": "Add 2-3 hands-on labs with code examples",
    "assess_lo_2_3": "Add mini-project or coding exercise"
  },
  "regenerate_modules": null  // only set if accept=false
}
```

ORCHESTRATOR RETRY LOOP
```
DO:
  outline = ModuleCreationAgent.run(input)
  feedback = ValidatorAgent.run(outline)
  if feedback.score >= 75:
    ACCEPT and return
  else:
    // Regenerate with targeted feedback
    input.targeted_feedback = feedback.targeted_edits
    iteration_count++
WHILE score < 75 AND iteration_count < 3
IF iteration_count >= 3:
  LOG warning, return best-scored outline anyway
```

EXIT CONDITION
âœ“ Validator scores outlines consistently (gold-set calibration)
âœ“ Low-quality outlines score < 75
âœ“ High-quality outlines score >= 90
âœ“ Feedback is targeted and actionable
âœ“ Regeneration loop works (max 3 retries)
âœ“ Loop terminates after max retries
âœ“ UI shows acceptance/rejection reason

TESTING
- test_phase_6_validator.py: Scoring consistency, feedback quality, loop termination

DURATION ESTIMATE: 6-7 days

---

ðŸŸ¢ PHASE 7 â€” Query Agent (Interactive Explanations)
==============================================================================

ðŸŽ¯ GOAL
Enable educators to ask follow-ups, explore reasoning, and request targeted changes.
Session-aware conversational interface.

DELIVERABLES
âœ“ Query Agent answers common follow-ups
âœ“ Provenance-attached responses
âœ“ Session context awareness
âœ“ Confidence scoring
âœ“ Optional regeneration signal (single module)

RESPONSIBILITIES
- Conversation Lead: Query design, context management
- Backend Lead: Query Agent, context retrieval
- Frontend Lead: Chat UI in Streamlit

KEY FILES
- agents/query_agent.py - Query Agent implementation
- app.py - Chat interface integration

SAMPLE QUERIES (And Expected Behavior)
```
"Why is Module 2 included?"
  â†’ Answer: "Module 2 (X) is included because your learning mode is 'Hybrid'
     and depth is 'Implementation'. The module prepares students for the capstone."
  Sources: [{'from': 'user_input.depth_requirement', ...}]
  Confidence: 0.9

"Can you simplify Module 3?"
  â†’ Answer: "I can regenerate Module 3 with fewer prerequisites and simpler activities."
  can_regenerate_module: "M_3"

"What resources influenced the assessment strategy?"
  â†’ Answer: "Assessment strategy is based on:
     1. Web source: https://... (Bloom's assessment best practices)
     2. Retrieved doc: XYZ Institution's rubric"
  Sources: [{'from': 'web', 'url': '...', 'confidence': 0.85}, ...]
  Confidence: 0.7
```

CONTEXT ACCESS
Query Agent has read-only access to:
- user_input (original request)
- retrieved_docs (from RAG)
- web_results (from web search)
- generated_outline (current)
- conversation_history (session)

EXIT CONDITION
âœ“ Queries are answered with context
âœ“ Sources are correctly cited (no hallucinations)
âœ“ Confidence scores reflect uncertainty
âœ“ Module regeneration can be triggered
âœ“ Session context preserved across queries

TESTING
- test_phase_7_query.py: Provenance accuracy, confidence calibration

DURATION ESTIMATE: 4-5 days

---

ðŸŸ¢ PHASE 8 â€” Streamlit UX Polish & Exports
==============================================================================

ðŸŽ¯ GOAL
Make the system educator-friendly and production-quality.

DELIVERABLES
âœ“ Editable outline sections (educator can tweak)
âœ“ Regenerate single-module button
âœ“ Multi-format exports (Markdown, PDF, JSON)
âœ“ Download links
âœ“ Feedback widget (optional: rate outline, comment)
âœ“ Preview pane
âœ“ Progress indicator during generation
âœ“ Error messages (graceful degradation)

RESPONSIBILITIES
- Frontend Lead: Full Streamlit UX
- Backend Lead: Export logic, error handling
- DevOps: Temp file cleanup, concurrency

KEY FILES
- app.py - Full UX implementation
- utils/session.py - Export helpers

EXPORT FORMATS
```
JSON: Full CourseOutlineSchema (can be re-imported)
Markdown: Readable format (for sharing, editing in Word/Notion)
PDF: Professional format (for printing, distribution)
```

EXIT CONDITION
âœ“ Educators can edit sections
âœ“ Regenerate button works
âœ“ All export formats work
âœ“ Exported files are valid (spot-check)
âœ“ Feedback reaches logging system
âœ“ Session cleans up after export

TESTING
- test_phase_8_ux.py: Export integrity, editing, buttons

DURATION ESTIMATE: 5-6 days

---

ðŸŸ¢ PHASE 9 â€” Observability, Monitoring & Metrics
==============================================================================

ðŸŽ¯ GOAL
Make the system production-ready: trackable, debuggable, improvable.

DELIVERABLES
âœ“ Structured logging (no PII)
âœ“ Agent latency tracking
âœ“ Validator score trends
âœ“ Regeneration frequency metrics
âœ“ Model token usage tracking
âœ“ Error rate monitoring
âœ“ Metrics dashboard (optional: simple Streamlit view)
âœ“ Audit trail (decisions, feedback, versions)

RESPONSIBILITIES
- DevOps / SRE Lead: Logging infrastructure, metrics export
- Backend Lead: Instrumentation across agents
- QA Lead: PII filtering, audit completeness

KEY FILES
- utils/logging.py - AudioLogger implementation
- app.py - Metrics dashboard

METRICS COLLECTED
```
Agent-level:
  - Name, duration_ms, input_tokens, output_tokens, success, error

Request-level:
  - session_id, user_input.audience_level, user_input.depth, 
    attempt_count, final_score, accepted, total_duration_ms

Quality-level:
  - validator_scores (trend), regeneration_frequency, 
    educator_feedback_rating (if submitted)

Error-level:
  - error_type, agent_name, timestamp, session_id
```

PII FILTERING
- No educator names
- No student data
- No PDF content
- No course titles if they contain sensitive info

DASHBOARD (Simple Streamlit View)
```
- Average validator score over time
- Regeneration frequency
- Error rate
- Agent latency distribution
- Educator satisfaction (if collected)
```

EXIT CONDITION
âœ“ All agents instrumented
âœ“ No PII in logs
âœ“ Metrics queryable
âœ“ Audit trail complete
âœ“ Dashboard functional

TESTING
- test_phase_9_observability.py: Logging accuracy, PII filtering, cleanup

DURATION ESTIMATE: 4-5 days

==============================================================================
ðŸ”· SPRINT MAPPING (Example 4-week sprints)
==============================================================================

SPRINT 1 (Week 1-2):
  âœ“ PHASE 0 (Foundation)
  âœ“ PHASE 1 (UI + Session)
  âœ“ PHASE 2 (Orchestrator v1)

SPRINT 2 (Week 3-4):
  âœ“ PHASE 3 (Retrieval + ChromaDB)
  âœ“ PHASE 4 (Web Search)

SPRINT 3 (Week 5-6):
  âœ“ PHASE 5 (Module Creation Agent)

SPRINT 4 (Week 7-8):
  âœ“ PHASE 6 (Validator + Loop)
  âœ“ PHASE 7 (Query Agent)

SPRINT 5 (Week 9-10):
  âœ“ PHASE 8 (UX Polish)
  âœ“ PHASE 9 (Observability)

SPRINT 6+ (Week 11+):
  âœ“ Integration testing
  âœ“ Stage deployment
  âœ“ Educator pilot
  âœ“ Production deployment

==============================================================================
ðŸ”· KEY GUARDRAILS
==============================================================================

1. EACH PHASE ADDS ONE CAPABILITY (No scope creep)
2. AGENTS ARE STATELESS & REPRODUCIBLE (no side effects outside session)
3. TESTS VALIDATE BOTH FUNCTIONALITY & CONSTRAINTS
4. PHASE N DOES NOT DEPEND ON PHASE N+1 IMPLEMENTATION
   (Phase 5 works even if Phase 6 validator is stubbed)
5. NO PII IN LOGS OR EXPORTS
6. SESSION DATA IS EPHEMERAL (auto-delete on completion)
7. EVERY EXTERNAL CALL (LLM, Web, DB) HAS TIMEOUT + RETRY LOGIC
8. VALIDATOR FEEDBACK IS EXPLAINABLE (educator can understand why)
9. ALL AGENT OUTPUTS CONFORM TO SCHEMA (no unvalidated outputs)
10. UNIT TESTS > INTEGRATION TESTS > E2E TESTS (pyramid)

==============================================================================
ðŸ”· SUCCESS CRITERIA (Overall)
==============================================================================

Beta-Ready:
âœ“ End-to-end flow works (input â†’ outline â†’ output)
âœ“ Validator + retry loop works (agentic behavior)
âœ“ Tests pass (>80% code coverage)
âœ“ Educator can export in 3 formats
âœ“ No PII leaks

Pilot-Ready:
âœ“ 5+ educators use system with satisfaction > 4/5
âœ“ Outlines accepted on first try >= 60% of the time
âœ“ No unrecovered agent errors
âœ“ Metrics dashboard is correct

Production-Ready:
âœ“ <5 minute latency (p95) from input to outline
âœ“ 99% uptime SLA
âœ“ Zero PII incidents
âœ“ Educator feedback loop established
âœ“ Cost optimization done (model choice, caching)

==============================================================================
"""